#pragma once
#include <unordered_map>
#include <string>

namespace shader_embed {
    static const std::unordered_map<std::string, std::string> shaders = {
        { "common/constants.wgsl", "const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"" },
        { "prepare_dispatch.wgsl", "// Prepare indirect dispatch arguments based on keypoint count\n"
"// Reads the keypoint count and computes workgroup counts for descriptor/orientation shaders\n"
"\n"
"struct KeypointHeader {\n"
"    count: atomic<u32>,\n"
"    pad1: u32,\n"
"    pad2: u32,\n"
"    pad3: u32\n"
"}\n"
"\n"
"// Two sets of dispatch args: \n"
"// - First 3 u32s: for orientation (1 keypoint per workgroup, 256 threads per WG)\n"
"// - Second 3 u32s: for descriptor (64 keypoints per workgroup)\n"
"struct DispatchArgs {\n"
"    // Orientation: uses wid.x + wid.y * 65535 indexing scheme\n"
"    ori_x: u32,\n"
"    ori_y: u32,\n"
"    ori_z: u32,\n"
"    // Descriptor: simple 1D dispatch, 64 keypoints per workgroup\n"
"    desc_x: u32,\n"
"    desc_y: u32,\n"
"    desc_z: u32\n"
"}\n"
"\n"
"@group(0) @binding(0) var<storage, read_write> keypoints: KeypointHeader;\n"
"@group(0) @binding(1) var<storage, read_write> dispatch_args: DispatchArgs;\n"
"\n"
"// Dispatching 1 thread as this kernel performs serial calculations for indirect dispatch arguments.\n"
"@compute @workgroup_size(1)\n"
"fn main() {\n"
"    let count = atomicLoad(&keypoints.count);\n"
"    \n"
"    // Orientation: 1 keypoint per workgroup, use 2D dispatch for large counts\n"
"    // Matches: let idx = wid.x + wid.y * 65535u in orientation.wgsl\n"
"    let ori_workgroups = max(count, 1u);\n"
"    if (ori_workgroups <= 65535u) {\n"
"        dispatch_args.ori_x = ori_workgroups;\n"
"        dispatch_args.ori_y = 1u;\n"
"    } else {\n"
"        dispatch_args.ori_x = 65535u;\n"
"        dispatch_args.ori_y = (ori_workgroups + 65534u) / 65535u;\n"
"    }\n"
"    dispatch_args.ori_z = 1u;\n"
"    \n"
"    // Descriptor: 64 keypoints per workgroup (workgroup_size(64))\n"
"    let desc_workgroups = (count + 63u) / 64u;\n"
"    dispatch_args.desc_x = max(desc_workgroups, 1u);\n"
"    dispatch_args.desc_y = 1u;\n"
"    dispatch_args.desc_z = 1u;\n"
"}\n"
"\n"
"" },
        { "common/structs.wgsl", "struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"" },
        { "default/blur_horizontal.wgsl", "struct Params {\n"
"    width: u32,\n"
"    height: u32,\n"
"    radius: u32,\n"
"    pad: u32,\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var inputTex: texture_2d<f32>;\n"
"@group(0) @binding(2) var outputTex: texture_storage_2d<rgba32float, write>;\n"
"@group(0) @binding(3) var<storage, read> kernel: array<f32>;\n"
"\n"
"// Constants for shared memory optimization.\n"
"// These are fixed to match the default 16x16 workgroup size.\n"
"const MAX_RADIUS: u32 = 16u;\n"
"const TILE_WIDTH: u32 = 16u;\n"
"const TILE_HEIGHT: u32 = 16u;\n"
"const CACHE_WIDTH: u32 = 48u; // TILE_WIDTH + 2 * MAX_RADIUS (16 + 32)\n"
"\n"
"var<workgroup> tile_cache: array<f32, 768>; // TILE_HEIGHT * CACHE_WIDTH (16 * 48)\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"// WARNING: If WG_SIZE_X or WG_SIZE_Y are changed via specialization constants,\n"
"// the shared memory tile_cache size must also be updated.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(16, 16)\n"
"fn main(\n"
"    @builtin(global_invocation_id) gid: vec3u,\n"
"    @builtin(local_invocation_id) lid: vec3u\n"
") {\n"
"    let radius = i32(params.radius);\n"
"    let lx = lid.x;\n"
"    let ly = lid.y;\n"
"    let gx = i32(gid.x);\n"
"    let gy = i32(gid.y);\n"
"    \n"
"    // Shared memory row baseline\n"
"    let row_offset = ly * CACHE_WIDTH;\n"
"    let center_idx = row_offset + MAX_RADIUS + lx;\n"
"    \n"
"    // Load central pixels and halos into shared memory.\n"
"    // Each thread in the 16x16 block is responsible for loading 3 pixels horizontally\n"
"    // to cover the 16 original pixels plus 16 pixels of halo on each side.\n"
"    \n"
"    // 1. Central pixel\n"
"    if (gx < i32(params.width) && gy < i32(params.height)) {\n"
"        tile_cache[center_idx] = textureLoad(inputTex, vec2i(gx, gy), 0).r;\n"
"    } else {\n"
"        tile_cache[center_idx] = 0.0;\n"
"    }\n"
"    \n"
"    // 2. Left halo (16 pixels to the left)\n"
"    let left_gx = gx - i32(TILE_WIDTH);\n"
"    tile_cache[center_idx - TILE_WIDTH] = textureLoad(inputTex, vec2i(clamp(left_gx, 0, i32(params.width) - 1), gy), 0).r;\n"
"    \n"
"    // 3. Right halo (16 pixels to the right)\n"
"    let right_gx = gx + i32(TILE_WIDTH);\n"
"    tile_cache[center_idx + TILE_WIDTH] = textureLoad(inputTex, vec2i(clamp(right_gx, 0, i32(params.width) - 1), gy), 0).r;\n"
"    \n"
"    // Synchronize to ensure all threads have finished loading into tile_cache\n"
"    workgroupBarrier();\n"
"    \n"
"    // Boundary check for computation\n"
"    if (gid.x >= params.width || gid.y >= params.height) { return; }\n"
"    \n"
"    var sum: f32 = 0.0;\n"
"    for (var i: i32 = -radius; i <= radius; i++) {\n"
"        // Access shared memory instead of textureLoad\n"
"        sum += tile_cache[i32(center_idx) + i] * kernel[u32(i + radius)];\n"
"    }\n"
"    \n"
"    textureStore(outputTex, vec2i(gid.xy), vec4f(sum, 0.0, 0.0, 1.0));\n"
"}\n"
"" },
        { "default/blur_vertical.wgsl", "struct Params {\n"
"    width: u32,\n"
"    height: u32,\n"
"    radius: u32,\n"
"    pad: u32,\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var inputTex: texture_2d<f32>;\n"
"@group(0) @binding(2) var outputTex: texture_storage_2d<rgba32float, write>;\n"
"@group(0) @binding(3) var<storage, read> kernel: array<f32>;\n"
"\n"
"// Constants for shared memory optimization.\n"
"// These are fixed to match the default 16x16 workgroup size.\n"
"const MAX_RADIUS: u32 = 16u;\n"
"const TILE_WIDTH: u32 = 16u;\n"
"const TILE_HEIGHT: u32 = 16u;\n"
"const CACHE_HEIGHT: u32 = 48u; // TILE_HEIGHT + 2 * MAX_RADIUS (16 + 32)\n"
"\n"
"var<workgroup> tile_cache: array<f32, 768>; // CACHE_HEIGHT * TILE_WIDTH (48 * 16)\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"// WARNING: If WG_SIZE_X or WG_SIZE_Y are changed via specialization constants,\n"
"// the shared memory tile_cache size must also be updated.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(16, 16)\n"
"fn main(\n"
"    @builtin(global_invocation_id) gid: vec3u,\n"
"    @builtin(local_invocation_id) lid: vec3u\n"
") {\n"
"    let radius = i32(params.radius);\n"
"    let lx = lid.x;\n"
"    let ly = lid.y;\n"
"    let gx = i32(gid.x);\n"
"    let gy = i32(gid.y);\n"
"    \n"
"    // Shared memory layout: [ly + MAX_RADIUS][lx]\n"
"    let center_idx = (ly + MAX_RADIUS) * TILE_WIDTH + lx;\n"
"    \n"
"    // Load central pixels and halos into shared memory.\n"
"    // Each thread in the 16x16 block is responsible for loading 3 pixels vertically.\n"
"    \n"
"    // 1. Central pixel\n"
"    if (gx < i32(params.width) && gy < i32(params.height)) {\n"
"        tile_cache[center_idx] = textureLoad(inputTex, vec2i(gx, gy), 0).r;\n"
"    } else {\n"
"        tile_cache[center_idx] = 0.0;\n"
"    }\n"
"    \n"
"    // 2. Top halo (16 pixels up)\n"
"    let top_gy = gy - i32(TILE_HEIGHT);\n"
"    tile_cache[center_idx - TILE_HEIGHT * TILE_WIDTH] = textureLoad(inputTex, vec2i(gx, clamp(top_gy, 0, i32(params.height) - 1)), 0).r;\n"
"    \n"
"    // 3. Bottom halo (16 pixels down)\n"
"    let bot_gy = gy + i32(TILE_HEIGHT);\n"
"    tile_cache[center_idx + TILE_HEIGHT * TILE_WIDTH] = textureLoad(inputTex, vec2i(gx, clamp(bot_gy, 0, i32(params.height) - 1)), 0).r;\n"
"    \n"
"    // Synchronize to ensure all threads have finished loading into tile_cache\n"
"    workgroupBarrier();\n"
"    \n"
"    // Boundary check for computation\n"
"    if (gid.x >= params.width || gid.y >= params.height) { return; }\n"
"    \n"
"    var sum: f32 = 0.0;\n"
"    for (var i: i32 = -radius; i <= radius; i++) {\n"
"        // Access shared memory instead of textureLoad\n"
"        sum += tile_cache[i32(center_idx) + i * i32(TILE_WIDTH)] * kernel[u32(i + radius)];\n"
"    }\n"
"    \n"
"    textureStore(outputTex, vec2i(gid.xy), vec4f(sum, 0.0, 0.0, 1.0));\n"
"}\n"
"" },
        { "default/descriptor.wgsl", "struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"struct DescriptorList {\n"
"    data: array<f32> // Flattened: [idx * DESC_DIM + i]\n"
"}\n"
"struct Params {\n"
"    width: i32, height: i32, octave: i32, pad: i32\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read_write> keypoints: KeypointList;\n"
"@group(0) @binding(2) var<storage, read_write> descriptors: DescriptorList;\n"
"@group(0) @binding(3) var tex1: texture_2d<f32>;\n"
"@group(0) @binding(4) var tex2: texture_2d<f32>;\n"
"@group(0) @binding(5) var tex3: texture_2d<f32>;\n"
"\n"
"fn getVal(s: i32, x: i32, y: i32) -> f32 {\n"
"    if (s == 1) { return textureLoad(tex1, vec2i(x, y), 0).r; }\n"
"    if (s == 2) { return textureLoad(tex2, vec2i(x, y), 0).r; }\n"
"    if (s == 3) { return textureLoad(tex3, vec2i(x, y), 0).r; }\n"
"    return 0.0;\n"
"}\n"
"\n"
"fn getValBilinear(s: i32, x: f32, y: f32) -> f32 {\n"
"    let x0 = i32(floor(x));\n"
"    let y0 = i32(floor(y));\n"
"    let wx = x - f32(x0);\n"
"    let wy = y - f32(y0);\n"
"    \n"
"    let v00 = getVal(s, x0, y0);\n"
"    let v10 = getVal(s, x0 + 1, y0);\n"
"    let v01 = getVal(s, x0, y0 + 1);\n"
"    let v11 = getVal(s, x0 + 1, y0 + 1);\n"
"    \n"
"    return mix(mix(v00, v10, wx), mix(v01, v11, wx), wy);\n"
"}\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE: u32 = 64u;\n"
"\n"
"// 64 threads is chosen for descriptor generation as it involves more registers per thread.\n"
"// This preserves high occupancy while allowing sufficient resources for trilinear interpolation.\n"
"@compute @workgroup_size(WG_SIZE)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let idx = gid.x;\n"
"    if (idx >= atomicLoad(&keypoints.count)) { return; }\n"
"\n"
"    let kp = keypoints.points[idx];\n"
"    if (i32(kp.octave) != params.octave) { return; }\n"
"\n"
"    let x = kp.x / pow(2.0, kp.octave);\n"
"    let y = kp.y / pow(2.0, kp.octave);\n"
"    let scale = i32(kp.scale);\n"
"    let angle = kp.orientation;\n"
"    let cos_t = cos(angle);\n"
"    let sin_t = sin(angle);\n"
"\n"
"    // Scale-dependent step size (match packed version)\n"
"    let sigma = SIGMA_BASE * pow(2.0, f32(scale) / SCALES_PER_OCTAVE);\n"
"    let step = DESC_STEP_FACTOR * sigma; \n"
"\n"
"    var desc = array<f32, DESC_DIM>();\n"
"    for (var k=0u; k<DESC_DIM; k++) { desc[k] = 0.0; }\n"
"    \n"
"    // Iterate over 16x16 window\n"
"    for (var r = -8; r < 8; r++) {\n"
"        for (var c = -8; c < 8; c++) {\n"
"            // Rotated grid with step scaling\n"
"            // Standard rotation: \n"
"            // x' = x*cos - y*sin\n"
"            // y' = x*sin + y*cos\n"
"            // Here c is x-axis, r is y-axis of the grid\n"
"            let rot_x = step * (f32(c)*cos_t - f32(r)*sin_t);\n"
"            let rot_y = step * (f32(c)*sin_t + f32(r)*cos_t);\n"
"            \n"
"            let sample_x = x + rot_x;\n"
"            let sample_y = y + rot_y;\n"
"            \n"
"            if (sample_x < 1.0 || sample_x >= f32(params.width) - 1.0 || \n"
"                sample_y < 1.0 || sample_y >= f32(params.height) - 1.0) { continue; }\n"
"            \n"
"            let dx = getValBilinear(scale, sample_x+1.0, sample_y) - getValBilinear(scale, sample_x-1.0, sample_y);\n"
"            let dy = getValBilinear(scale, sample_x, sample_y+1.0) - getValBilinear(scale, sample_x, sample_y-1.0);\n"
"            \n"
"            let mag = sqrt(dx*dx + dy*dy);\n"
"            let ori = atan2(dy, dx) - angle;\n"
"            \n"
"            var n_ori = ori;\n"
"            while (n_ori < 0.0) { n_ori += TWO_PI; }\n"
"            while (n_ori >= TWO_PI) { n_ori -= TWO_PI; }\n"
"            \n"
"            // Trilinear Interpolation\n"
"            let rbin = (f32(r) + 8.0) / f32(DESC_SUBGRID_SIZE) - 0.5;\n"
"            let cbin = (f32(c) + 8.0) / f32(DESC_SUBGRID_SIZE) - 0.5;\n"
"            let obin = n_ori * f32(DESC_BINS) / TWO_PI;\n"
"            \n"
"            let mag_w = mag * exp(-(f32(r*r + c*c)) / DESC_GAUSSIAN_WEIGHT_SIGMA_SQ);\n"
"            \n"
"            for (var dr = 0; dr < 2; dr++) {\n"
"                let ri = i32(floor(rbin)) + dr;\n"
"                if (ri >= 0 && ri < 4) {\n"
"                    let r_w = select(1.0 - fract(rbin), fract(rbin), dr == 1);\n"
"                    \n"
"                    for (var dc = 0; dc < 2; dc++) {\n"
"                        let ci = i32(floor(cbin)) + dc;\n"
"                        if (ci >= 0 && ci < 4) {\n"
"                            let c_w = select(1.0 - fract(cbin), fract(cbin), dc == 1);\n"
"                            \n"
"                            for (var do_idx = 0; do_idx < 2; do_idx++) {\n"
"                                let oi_raw = i32(floor(obin)) + do_idx;\n"
"                                let o_w = select(1.0 - fract(obin), fract(obin), do_idx == 1);\n"
"                                \n"
"                                let oi = (oi_raw + i32(DESC_BINS)) % i32(DESC_BINS);\n"
"                                let idx = (ri * i32(DESC_SUBGRID_SIZE) + ci) * i32(DESC_BINS) + oi;\n"
"                                desc[idx] += mag_w * r_w * c_w * o_w;\n"
"                            }\n"
"                        }\n"
"                    }\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"    \n"
"    // Normalize\n"
"    var norm = 0.0;\n"
"    for (var k=0u; k<DESC_DIM; k++) { norm += desc[k]*desc[k]; }\n"
"    norm = sqrt(norm) + 0.00001;\n"
"    \n"
"    for (var k=0u; k<DESC_DIM; k++) {\n"
"        desc[k] = min(0.2, desc[k] / norm);\n"
"    }\n"
"    \n"
"    // Re-normalize\n"
"    norm = 0.0;\n"
"    for (var k=0u; k<DESC_DIM; k++) { norm += desc[k]*desc[k]; }\n"
"    norm = sqrt(norm) + 0.00001;\n"
"    \n"
"    for (var k=0u; k<DESC_DIM; k++) {\n"
"        descriptors.data[idx * DESC_DIM + k] = desc[k] / norm;\n"
"    }\n"
"}\n"
"" },
        { "default/dog.wgsl", "@group(0) @binding(0) var texA: texture_2d<f32>;\n"
"@group(0) @binding(1) var texB: texture_2d<f32>;\n"
"@group(0) @binding(2) var outputTex: texture_storage_2d<rgba32float, write>;\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(WG_SIZE_X, WG_SIZE_Y)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let dims = textureDimensions(texA);\n"
"    if (gid.x >= dims.x || gid.y >= dims.y) { return; }\n"
"    \n"
"    let a = textureLoad(texA, vec2i(gid.xy), 0).r;\n"
"    let b = textureLoad(texB, vec2i(gid.xy), 0).r;\n"
"    \n"
"    textureStore(outputTex, vec2i(gid.xy), vec4f(a - b, 0.0, 0.0, 1.0));\n"
"}\n"
"" },
        { "default/downsample.wgsl", "struct Params {\n"
"    srcWidth: u32,\n"
"    srcHeight: u32,\n"
"    dstWidth: u32,\n"
"    dstHeight: u32,\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var inputTex: texture_2d<f32>;\n"
"@group(0) @binding(2) var outputTex: texture_storage_2d<rgba32float, write>;\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(WG_SIZE_X, WG_SIZE_Y)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    if (gid.x >= params.dstWidth || gid.y >= params.dstHeight) { return; }\n"
"    \n"
"    let srcX = i32(gid.x * 2u);\n"
"    let srcY = i32(gid.y * 2u);\n"
"    \n"
"    let val = textureLoad(inputTex, vec2i(srcX, srcY), 0).r;\n"
"    textureStore(outputTex, vec2i(gid.xy), vec4f(val, 0.0, 0.0, 1.0));\n"
"}\n"
"" },
        { "default/extrema.wgsl", "struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"\n"
"struct Params {\n"
"    width: i32, height: i32, octave: i32, scale: i32, threshold: f32, edgeThreshold: f32\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var prevTex: texture_2d<f32>;\n"
"@group(0) @binding(2) var currTex: texture_2d<f32>;\n"
"@group(0) @binding(3) var nextTex: texture_2d<f32>;\n"
"@group(0) @binding(4) var<storage, read_write> keypoints: KeypointList;\n"
"\n"
"var<workgroup> wgCount: atomic<u32>;\n"
"var<workgroup> wgGlobalOffset: u32;\n"
"\n"
"fn getVal(tex: texture_2d<f32>, x: i32, y: i32) -> f32 {\n"
"    return textureLoad(tex, vec2i(x, y), 0).r;\n"
"}\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(WG_SIZE_X, WG_SIZE_Y)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u, @builtin(local_invocation_index) lid: u32) {\n"
"    // Init workgroup atomic\n"
"    if (lid == 0u) {\n"
"        atomicStore(&wgCount, 0u);\n"
"    }\n"
"    workgroupBarrier();\n"
"\n"
"    let x = i32(gid.x);\n"
"    let y = i32(gid.y);\n"
"    \n"
"    var isFeature = false;\n"
"    \n"
"    if (x >= 1 && y >= 1 && x < params.width - 1 && y < params.height - 1) {\n"
"        let val = getVal(currTex, x, y);\n"
"        if (abs(val) >= params.threshold) {\n"
"            var isMax = true;\n"
"            var isMin = true;\n"
"            // Checks...\n"
"            for (var vz = -1; vz <= 1; vz++) {\n"
"                for (var vy = -1; vy <= 1; vy++) {\n"
"                    for (var vx = -1; vx <= 1; vx++) {\n"
"                        if (vx == 0 && vy == 0 && vz == 0) { continue; }\n"
"                        var neighborVal: f32;\n"
"                        if (vz == -1) { neighborVal = getVal(prevTex, x + vx, y + vy); }\n"
"                        else if (vz == 0) { neighborVal = getVal(currTex, x + vx, y + vy); }\n"
"                        else { neighborVal = getVal(nextTex, x + vx, y + vy); }\n"
"                        \n"
"                        if (neighborVal >= val) { isMax = false; }\n"
"                        if (neighborVal <= val) { isMin = false; }\n"
"                        \n"
"                        if (!isMax && !isMin) { break; } \n"
"                    }\n"
"                    if (!isMax && !isMin) { break; }\n"
"                }\n"
"                if (!isMax && !isMin) { break; }\n"
"            }\n"
"            \n"
"            if (isMax || isMin) {\n"
"                // Edge check\n"
"                let dxx = getVal(currTex, x+1, y) + getVal(currTex, x-1, y) - 2.0 * val;\n"
"                let dyy = getVal(currTex, x, y+1) + getVal(currTex, x, y-1) - 2.0 * val;\n"
"                let dxy = (getVal(currTex, x+1, y+1) - getVal(currTex, x+1, y-1) - getVal(currTex, x-1, y+1) + getVal(currTex, x-1, y-1)) * 0.25;\n"
"                // Edge check: Reject points that have a large principal curvature in one direction\n"
"                // but a small one in the other (edges).\n"
"                // Uses the ratio of eigenvalues of the 2x2 Hessian matrix.\n"
"                let tr = dxx + dyy;\n"
"                let det = dxx * dyy - dxy * dxy;\n"
"                let r = params.edgeThreshold;\n"
"                \n"
"                if (det > 0.0 && (tr * tr * r) < ((r + 1.0) * (r + 1.0) * det)) {\n"
"                    isFeature = true;\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"\n"
"    // Aggregation\n"
"    var myWgIdx = 0u;\n"
"    if (isFeature) {\n"
"        myWgIdx = atomicAdd(&wgCount, 1u);\n"
"    }\n"
"    workgroupBarrier();\n"
"\n"
"    if (lid == 0u) {\n"
"        let count = atomicLoad(&wgCount);\n"
"        if (count > 0u) {\n"
"            wgGlobalOffset = atomicAdd(&keypoints.count, count);\n"
"        }\n"
"    }\n"
"    workgroupBarrier();\n"
"\n"
"    if (isFeature) {\n"
"        let idx = wgGlobalOffset + myWgIdx;\n"
"        // Write keypoint\n"
"        let scaleMult = pow(2.0, f32(params.octave));\n"
"        keypoints.points[idx].x = f32(x) * scaleMult;\n"
"        keypoints.points[idx].y = f32(y) * scaleMult;\n"
"        keypoints.points[idx].octave = f32(params.octave);\n"
"        keypoints.points[idx].scale = f32(params.scale);\n"
"        keypoints.points[idx].sigma = SIGMA_BASE * pow(2.0, (f32(params.scale) / SCALES_PER_OCTAVE)) * scaleMult;\n"
"        keypoints.points[idx].orientation = 0.0;\n"
"    }\n"
"}\n"
"" },
        { "default/grayscale.wgsl", "@group(0) @binding(0) var inputTex: texture_2d<f32>;\n"
"@group(0) @binding(1) var outputTex: texture_storage_2d<rgba32float, write>;\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(WG_SIZE_X, WG_SIZE_Y)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let dims = textureDimensions(inputTex);\n"
"    if (gid.x >= dims.x || gid.y >= dims.y) { return; }\n"
"    \n"
"    let color = textureLoad(inputTex, vec2i(gid.xy), 0);\n"
"    let gray = dot(color.rgb, vec3f(0.299, 0.587, 0.114));\n"
"    \n"
"    textureStore(outputTex, vec2i(gid.xy), vec4f(gray, 0.0, 0.0, 1.0));\n"
"}\n"
"" },
        { "default/matcher.wgsl", "struct Params {\n"
"    countA: u32,\n"
"    countB: u32,\n"
"    pad1: u32,\n"
"    pad2: u32,\n"
"}\n"
"\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDist: f32,\n"
"    secondDist: f32,\n"
"    pad: f32,\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read> listA: array<f32>; // 128 floats per desc\n"
"@group(0) @binding(2) var<storage, read> listB: array<f32>; // 128 floats per desc\n"
"@group(0) @binding(3) var<storage, read_write> results: array<MatchResult>;\n"
"\n"
"@compute @workgroup_size(64)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let idxA = gid.x;\n"
"    if (idxA >= params.countA) { return; }\n"
"\n"
"    // Load descriptor A\n"
"    var descA: array<f32, 128>;\n"
"    for (var k = 0u; k < 128u; k++) {\n"
"        descA[k] = listA[idxA * 128u + k];\n"
"    }\n"
"\n"
"    var bestDist = 1e30;\n"
"    var secondDist = 1e30;\n"
"    var bestIdx = -1;\n"
"\n"
"    // Brute force search list B\n"
"    for (var idxB = 0u; idxB < params.countB; idxB++) {\n"
"        var distSq = 0.0;\n"
"        for (var k = 0u; k < 128u; k++) {\n"
"            let diff = descA[k] - listB[idxB * 128u + k];\n"
"            distSq += diff * diff;\n"
"        }\n"
"\n"
"        // Update top 2\n"
"        if (distSq < bestDist) {\n"
"            secondDist = bestDist;\n"
"            bestDist = distSq;\n"
"            bestIdx = i32(idxB);\n"
"        } else if (distSq < secondDist) {\n"
"            secondDist = distSq;\n"
"        }\n"
"    }\n"
"\n"
"    // Write result \n"
"    results[idxA].bestIdx = bestIdx;\n"
"    results[idxA].bestDist = bestDist;\n"
"    results[idxA].secondDist = secondDist;\n"
"}\n"
"" },
        { "default/orientation.wgsl", "struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"\n"
"struct Params {\n"
"    width: i32, height: i32, octave: i32\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read_write> keypoints: KeypointList;\n"
"@group(0) @binding(2) var tex1: texture_2d<f32>;\n"
"@group(0) @binding(3) var tex2: texture_2d<f32>;\n"
"@group(0) @binding(4) var tex3: texture_2d<f32>;\n"
"\n"
"var<workgroup> wgHist: array<atomic<u32>, ORI_BINS>;\n"
"\n"
"fn getVal(s: i32, x: i32, y: i32) -> f32 {\n"
"    if (s == 1) { return textureLoad(tex1, vec2i(x, y), 0).r; }\n"
"    if (s == 2) { return textureLoad(tex2, vec2i(x, y), 0).r; }\n"
"    if (s == 3) { return textureLoad(tex3, vec2i(x, y), 0).r; }\n"
"    return 0.0;\n"
"}\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE: u32 = 256u;\n"
"\n"
"// 256 threads (1D) provides high occupancy and matches the thread count of 2D kernels (16x16).\n"
"// This is suitable for processing lists of keypoints.\n"
"@compute @workgroup_size(WG_SIZE)\n"
"fn main(@builtin(workgroup_id) wid: vec3u, @builtin(local_invocation_index) lid: u32) {\n"
"    let idx = wid.x + wid.y * 65535u;\n"
"    \n"
"    // Init shared hist\n"
"    if (lid < ORI_BINS) {\n"
"        atomicStore(&wgHist[lid], 0u);\n"
"    }\n"
"    workgroupBarrier();\n"
"\n"
"    let idxValid = idx < atomicLoad(&keypoints.count);\n"
"    var kp_octave = 0.0;\n"
"    var kp_x = 0.0;\n"
"    var kp_y = 0.0;\n"
"    var kp_scale = 0.0;\n"
"    \n"
"    if (idxValid) {\n"
"        let kp = keypoints.points[idx];\n"
"        kp_octave = kp.octave;\n"
"        kp_x = kp.x;\n"
"        kp_y = kp.y;\n"
"        kp_scale = kp.scale;\n"
"    }\n"
"    \n"
"    let isValid = idxValid && (i32(kp_octave) == params.octave);\n"
"\n"
"    var x = 0;\n"
"    var y = 0;\n"
"    var scale = 0;\n"
"    var sigma = 0.0;\n"
"    var radius = 0;\n"
"    var radiusSq = 0.0;\n"
"    var width = 0;\n"
"    var totalPixels = 0;\n"
"    \n"
"    if (isValid) {\n"
"        x = i32(round(kp_x / pow(2.0, kp_octave)));\n"
"        y = i32(round(kp_y / pow(2.0, kp_octave)));\n"
"        scale = i32(kp_scale);\n"
"        \n"
"        sigma = SIGMA_BASE * pow(2.0, f32(scale) / SCALES_PER_OCTAVE); \n"
"        // CPU: Math.round(localSigma * 1.5 * 3)\n"
"        // GPU: ceil(sigma * 1.5 * 3.0) -> Changed to round to match\n"
"        radius = i32(round(sigma * 1.5 * 3.0));\n"
"        radiusSq = f32(radius * radius);\n"
"        width = 2 * radius + 1;\n"
"        totalPixels = width * width;\n"
"    }\n"
"    \n"
"    if (isValid && totalPixels > 0) {\n"
"        for (var i = i32(lid); i < totalPixels; i += 256) {\n"
"            let dy = (i / width) - radius;\n"
"            let dx = (i % width) - radius;\n"
"            \n"
"            let r2 = f32(dx*dx + dy*dy);\n"
"            if (r2 <= radiusSq) {\n"
"                let px = x + dx;\n"
"                let py = y + dy;\n"
"                // Check bounds\n"
"                if (px >= 1 && px < params.width - 1 && py >= 1 && py < params.height - 1) {\n"
"                    let rx = getVal(scale, px+1, py) - getVal(scale, px-1, py);\n"
"                    let ry = getVal(scale, px, py+1) - getVal(scale, px, py-1);\n"
"                    let mag = sqrt(rx*rx + ry*ry);\n"
"                    let sigma_w = 1.5 * sigma;\n"
"                    let weight = exp(-r2 / (2.0 * sigma_w * sigma_w));\n"
"                    \n"
"                    let ang_raw = atan2(ry, rx);\n"
"                    let ang = select(ang_raw, ang_raw + TWO_PI, ang_raw < 0.0);\n"
"                    let bin = i32(floor(ang * f32(ORI_BINS) / TWO_PI)) % i32(ORI_BINS);\n"
"                    \n"
"                    atomicAdd(&wgHist[bin], u32(mag * weight * HIST_SCALE));\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"    workgroupBarrier();\n"
"\n"
"    // Find max (Thread 0)\n"
"    if (lid == 0u && isValid) {\n"
"        // Read histogram\n"
"        var rawHist = array<f32, ORI_BINS>();\n"
"        for (var i = 0; i < i32(ORI_BINS); i++) {\n"
"            rawHist[i] = f32(atomicLoad(&wgHist[i])) / HIST_SCALE;\n"
"        }\n"
"\n"
"        // Smooth Histogram [0.25, 0.5, 0.25]\n"
"        var histFloats = array<f32, ORI_BINS>();\n"
"        for (var i = 0; i < i32(ORI_BINS); i++) {\n"
"            let prev = rawHist[(i + i32(ORI_BINS) - 1) % i32(ORI_BINS)];\n"
"            let curr = rawHist[i];\n"
"            let next = rawHist[(i + 1) % i32(ORI_BINS)];\n"
"            histFloats[i] = 0.25 * prev + 0.5 * curr + 0.25 * next;\n"
"        }\n"
"\n"
"        var maxVal = -1.0;\n"
"        var maxBin = 0;\n"
"        \n"
"        for (var i = 0; i < i32(ORI_BINS); i++) {\n"
"            let val = histFloats[i];\n"
"            if (val > maxVal) {\n"
"                maxVal = val;\n"
"                maxBin = i;\n"
"            }\n"
"        }\n"
"        \n"
"        // Refine\n"
"        let left = histFloats[(maxBin + i32(ORI_BINS) - 1) % i32(ORI_BINS)];\n"
"        let right = histFloats[(maxBin + 1) % i32(ORI_BINS)];\n"
"        let peak = f32(maxBin) + 0.5 * (left - right) / (left - 2.0 * maxVal + right);\n"
"        let orientation = peak * TWO_PI / f32(ORI_BINS);\n"
"        \n"
"        keypoints.points[idx].orientation = orientation;\n"
"    }\n"
"}\n"
"" },
        { "packed/blur_horizontal.wgsl", "struct Params {\n"
"    width: u32,  // Packed width\n"
"    height: u32, // Packed height\n"
"    radius: u32,\n"
"    pad: u32,\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var inputTex: texture_2d<f32>; // Packed\n"
"@group(0) @binding(2) var outputTex: texture_storage_2d<rgba32float, write>; // Packed\n"
"@group(0) @binding(3) var<storage, read> kernel: array<f32>;\n"
"\n"
"// Unpack helper: Get logical pixel value at (lx, ly) \n"
"// knowing that (px, py) contains [ (2px, 2py), (2px+1, 2py), (2px, 2py+1), (2px+1, 2py+1) ]\n"
"// But fetching randomly is slow. We should iterate intelligently.\n"
"\n"
"// Constants for shared memory optimization.\n"
"// These are fixed to match the default 16x16 workgroup size.\n"
"const MAX_RADIUS_PACKED: u32 = 16u;\n"
"const TILE_WIDTH_PACKED: u32 = 16u;\n"
"const TILE_HEIGHT_PACKED: u32 = 16u;\n"
"const CACHE_WIDTH_PACKED: u32 = 48u; // TILE_WIDTH_PACKED + 2 * MAX_RADIUS_PACKED\n"
"\n"
"var<workgroup> tile_cache: array<vec4f, 768>; // TILE_HEIGHT_PACKED * CACHE_WIDTH_PACKED (16 * 48)\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(16, 16)\n"
"fn main(\n"
"    @builtin(global_invocation_id) gid: vec3u,\n"
"    @builtin(local_invocation_id) lid: vec3u\n"
") {\n"
"    let px = i32(gid.x);\n"
"    let py = i32(gid.y);\n"
"    let lx = lid.x;\n"
"    let ly = lid.y;\n"
"    let radius = i32(params.radius);\n"
"\n"
"    // Shared memory row baseline\n"
"    let row_offset = ly * CACHE_WIDTH_PACKED;\n"
"    let center_idx = row_offset + MAX_RADIUS_PACKED + lx;\n"
"\n"
"    // Load packed texels (vec4f) into shared memory.\n"
"    // Each packed texel contains 4 logical pixels.\n"
"    \n"
"    // 1. Central packed texel\n"
"    if (px < i32(params.width) && py < i32(params.height)) {\n"
"        tile_cache[center_idx] = textureLoad(inputTex, vec2i(px, py), 0);\n"
"    } else {\n"
"        tile_cache[center_idx] = vec4f(0.0);\n"
"    }\n"
"\n"
"    // 2. Left halo (16 packed texels left)\n"
"    let left_px = px - i32(TILE_WIDTH_PACKED);\n"
"    tile_cache[center_idx - TILE_WIDTH_PACKED] = textureLoad(inputTex, vec2i(clamp(left_px, 0, i32(params.width) - 1), py), 0);\n"
"    \n"
"    // 3. Right halo (16 packed texels right)\n"
"    let right_px = px + i32(TILE_WIDTH_PACKED);\n"
"    tile_cache[center_idx + TILE_WIDTH_PACKED] = textureLoad(inputTex, vec2i(clamp(right_px, 0, i32(params.width) - 1), py), 0);\n"
"\n"
"    // Synchronize to ensure all threads have finished loading into tile_cache\n"
"    workgroupBarrier();\n"
"\n"
"    if (gid.x >= params.width || gid.y >= params.height) { return; }\n"
"    \n"
"    var sumRow0_0 = 0.0; // For pixel (2px, 2py)\n"
"    var sumRow0_1 = 0.0; // For pixel (2px+1, 2py)\n"
"    var sumRow1_0 = 0.0; // For pixel (2px, 2py+1)\n"
"    var sumRow1_1 = 0.0; // For pixel (2px+1, 2py+1)\n"
"    \n"
"    for (var k = -radius; k <= radius; k++) {\n"
"        let weight = kernel[u32(k + radius)];\n"
"        \n"
"        // --- Row 0 ---\n"
"        // Target 0: sx = 2*px. Neighbor = 2*px + k.\n"
"        // Target 1: sx = 2*px+1. Neighbor = 2*px + 1 + k.\n"
"        let lx0 = clamp(px * 2 + k, 0, i32(params.width) * 2 - 1);\n"
"        let lx1 = clamp(px * 2 + 1 + k, 0, i32(params.width) * 2 - 1);\n"
"        \n"
"        // Fetch values from shared memory cache\n"
"        // Packed relative offset: p_sx_rel = (lx0 / 2) - px\n"
"        let p0_x_rel = (lx0 / 2) - px;\n"
"        let p0_mod = lx0 % 2;\n"
"        let val0_packed = tile_cache[i32(center_idx) + p0_x_rel];\n"
"        let val0 = select(val0_packed.x, val0_packed.y, p0_mod == 1);\n"
"        sumRow0_0 += val0 * weight;\n"
"        sumRow1_0 += select(val0_packed.z, val0_packed.w, p0_mod == 1) * weight;\n"
"        \n"
"        let p1_x_rel = (lx1 / 2) - px;\n"
"        let p1_mod = lx1 % 2;\n"
"        let val1_packed = tile_cache[i32(center_idx) + p1_x_rel];\n"
"        let val1 = select(val1_packed.x, val1_packed.y, p1_mod == 1);\n"
"        sumRow0_1 += val1 * weight;\n"
"        sumRow1_1 += select(val1_packed.z, val1_packed.w, p1_mod == 1) * weight;\n"
"    }\n"
"    \n"
"    // Store Result\n"
"    textureStore(outputTex, vec2i(px, py), vec4f(sumRow0_0, sumRow0_1, sumRow1_0, sumRow1_1));\n"
"}\n"
"" },
        { "packed/blur_vertical.wgsl", "struct Params {\n"
"    width: u32,  // Packed width\n"
"    height: u32, // Packed height\n"
"    radius: u32,\n"
"    pad: u32,\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var inputTex: texture_2d<f32>; // Packed\n"
"@group(0) @binding(2) var outputTex: texture_storage_2d<rgba32float, write>; // Packed\n"
"@group(0) @binding(3) var<storage, read> kernel: array<f32>;\n"
"\n"
"// Constants for shared memory optimization.\n"
"// These are fixed to match the default 16x16 workgroup size.\n"
"const MAX_RADIUS_PACKED: u32 = 16u;\n"
"const TILE_WIDTH_PACKED: u32 = 16u;\n"
"const TILE_HEIGHT_PACKED: u32 = 16u;\n"
"const CACHE_HEIGHT_PACKED: u32 = 48u; // TILE_HEIGHT_PACKED + 2 * MAX_RADIUS_PACKED\n"
"\n"
"var<workgroup> tile_cache: array<vec4f, 768>; // CACHE_HEIGHT_PACKED * TILE_WIDTH_PACKED (48 * 16)\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(16, 16)\n"
"fn main(\n"
"    @builtin(global_invocation_id) gid: vec3u,\n"
"    @builtin(local_invocation_id) lid: vec3u\n"
") {\n"
"    let px = i32(gid.x);\n"
"    let py = i32(gid.y);\n"
"    let lx = lid.x;\n"
"    let ly = lid.y;\n"
"    let radius = i32(params.radius);\n"
"    \n"
"    // Shared memory layout: [ly + MAX_RADIUS_PACKED][lx]\n"
"    let center_idx = (ly + MAX_RADIUS_PACKED) * TILE_WIDTH_PACKED + lx;\n"
"    \n"
"    // Load packed texels (vec4f) into shared memory.\n"
"    \n"
"    // 1. Central packed texel\n"
"    if (px < i32(params.width) && py < i32(params.height)) {\n"
"        tile_cache[center_idx] = textureLoad(inputTex, vec2i(px, py), 0);\n"
"    } else {\n"
"        tile_cache[center_idx] = vec4f(0.0);\n"
"    }\n"
"    \n"
"    // 2. Top halo (16 packed texels up)\n"
"    let top_py = py - i32(TILE_HEIGHT_PACKED);\n"
"    tile_cache[center_idx - TILE_HEIGHT_PACKED * TILE_WIDTH_PACKED] = textureLoad(inputTex, vec2i(px, clamp(top_py, 0, i32(params.height) - 1)), 0);\n"
"    \n"
"    // 3. Bottom halo (16 packed texels down)\n"
"    let bot_py = py + i32(TILE_HEIGHT_PACKED);\n"
"    tile_cache[center_idx + TILE_HEIGHT_PACKED * TILE_WIDTH_PACKED] = textureLoad(inputTex, vec2i(px, clamp(bot_py, 0, i32(params.height) - 1)), 0);\n"
"    \n"
"    // Synchronize to ensure all threads have finished loading into tile_cache\n"
"    workgroupBarrier();\n"
"    \n"
"    if (gid.x >= params.width || gid.y >= params.height) { return; }\n"
"    \n"
"    var sumCol0_0 = 0.0; // (2px, 2py)\n"
"    var sumCol0_1 = 0.0; // (2px, 2py+1)\n"
"    var sumCol1_0 = 0.0; // (2px+1, 2py)\n"
"    var sumCol1_1 = 0.0; // (2px+1, 2py+1)\n"
"    \n"
"    for (var k = -radius; k <= radius; k++) {\n"
"        let weight = kernel[u32(k + radius)];\n"
"\n"
"        // Logical Y coords\n"
"        let ly0 = clamp(py * 2 + k, 0, i32(params.height) * 2 - 1);\n"
"        let ly1 = clamp(py * 2 + 1 + k, 0, i32(params.height) * 2 - 1);\n"
"\n"
"        // Fetch values from shared memory cache\n"
"        // Packed relative offset: p_sy_rel = (ly0 / 2) - py\n"
"        let py0_rel = (ly0 / 2) - py;\n"
"        let py0_mod = ly0 % 2; // 0 (top/xy) or 1 (bot/zw)\n"
"        let v0 = tile_cache[i32(center_idx) + py0_rel * i32(TILE_WIDTH_PACKED)];\n"
"        \n"
"        let py1_rel = (ly1 / 2) - py;\n"
"        let py1_mod = ly1 % 2;\n"
"        let v1 = tile_cache[i32(center_idx) + py1_rel * i32(TILE_WIDTH_PACKED)];\n"
"        \n"
"        sumCol0_0 += select(v0.x, v0.z, py0_mod == 1) * weight;\n"
"        sumCol0_1 += select(v1.x, v1.z, py1_mod == 1) * weight;\n"
"        sumCol1_0 += select(v0.y, v0.w, py0_mod == 1) * weight;\n"
"        sumCol1_1 += select(v1.y, v1.w, py1_mod == 1) * weight;\n"
"    }\n"
"    \n"
"    // Output: (SumC0_0, SumC1_0, SumC0_1, SumC1_1)\n"
"    textureStore(outputTex, vec2i(px, py), vec4f(sumCol0_0, sumCol1_0, sumCol0_1, sumCol1_1));\n"
"}\n"
"" },
        { "packed/descriptor.wgsl", "struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"struct DescriptorList {\n"
"    data: array<f32>\n"
"}\n"
"struct Params {\n"
"    width: i32, height: i32, octave: i32, pad: i32\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read_write> keypoints: KeypointList;\n"
"@group(0) @binding(2) var<storage, read_write> descriptors: DescriptorList;\n"
"@group(0) @binding(3) var tex1: texture_2d<f32>;\n"
"@group(0) @binding(4) var tex2: texture_2d<f32>;\n"
"@group(0) @binding(5) var tex3: texture_2d<f32>;\n"
"\n"
"fn getVal(s: i32, lx: i32, ly: i32) -> f32 {\n"
"    let px = lx / 2;\n"
"    let py = ly / 2;\n"
"    let mx = lx % 2;\n"
"    let my = ly % 2;\n"
"    \n"
"    var val: vec4f;\n"
"    if (s == 1) { val = textureLoad(tex1, vec2i(px, py), 0); }\n"
"    else if (s == 2) { val = textureLoad(tex2, vec2i(px, py), 0); }\n"
"    else { val = textureLoad(tex3, vec2i(px, py), 0); }\n"
"    \n"
"    if (my == 0) {\n"
"        return select(val.x, val.y, mx == 1);\n"
"    } else {\n"
"        return select(val.z, val.w, mx == 1);\n"
"    }\n"
"}\n"
"\n"
"\n"
"fn getValBilinear(s: i32, x: f32, y: f32) -> f32 {\n"
"    let x0 = i32(floor(x));\n"
"    let y0 = i32(floor(y));\n"
"    let x1 = x0 + 1;\n"
"    let y1 = y0 + 1;\n"
"    let wx = x - f32(x0);\n"
"    let wy = y - f32(y0);\n"
"    \n"
"    // Bounds check handled by getVal implicitly (textureLoad safe? No, should be careful)\n"
"    // But descriptor loop checks bounds.\n"
"    \n"
"    let v00 = getVal(s, x0, y0);\n"
"    let v10 = getVal(s, x1, y0);\n"
"    let v01 = getVal(s, x0, y1);\n"
"    let v11 = getVal(s, x1, y1);\n"
"    \n"
"    return mix(mix(v00, v10, wx), mix(v01, v11, wx), wy);\n"
"}\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE: u32 = 64u;\n"
"\n"
"// 64 threads is chosen for descriptor generation as it involves more registers per thread.\n"
"// This preserves high occupancy while allowing sufficient resources for trilinear interpolation.\n"
"@compute @workgroup_size(WG_SIZE)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let idx = gid.x;\n"
"    if (idx >= atomicLoad(&keypoints.count)) { return; }\n"
"\n"
"    let kp = keypoints.points[idx];\n"
"    if (i32(kp.octave) != params.octave) { return; }\n"
"\n"
"    let x = kp.x / pow(2.0, kp.octave);\n"
"    let y = kp.y / pow(2.0, kp.octave);\n"
"    let scale = i32(kp.scale);\n"
"    let angle = kp.orientation;\n"
"    let cos_t = cos(angle);\n"
"    let sin_t = sin(angle);\n"
"    \n"
"    // Scale-dependent step size\n"
"    let sigma = SIGMA_BASE * pow(2.0, f32(scale) / SCALES_PER_OCTAVE);\n"
"    let step = DESC_STEP_FACTOR * sigma; // 16 samples covers ~12 sigma\n"
"\n"
"    var desc = array<f32, DESC_DIM>();\n"
"    for (var k=0u; k<DESC_DIM; k++) { desc[k] = 0.0; }\n"
"    \n"
"\n"
"    for (var r = -8; r < 8; r++) {\n"
"        for (var c = -8; c < 8; c++) {\n"
"            // Revert to Original Rotation Logic but with Step Scaling\n"
"            // Fixed rotation application\n"
"            let rot_x = step * (f32(c)*cos_t - f32(r)*sin_t);\n"
"            let rot_y = step * (f32(c)*sin_t + f32(r)*cos_t);\n"
"            \n"
"            let sample_x = x + rot_x; // Keep as f32\n"
"            let sample_y = y + rot_y; // Keep as f32\n"
"            \n"
"            // Bounds check (ensure x+1/y+1 are valid)\n"
"            if (sample_x < 2.0 || sample_x >= f32(params.width * 2) - 2.0 || \n"
"                sample_y < 2.0 || sample_y >= f32(params.height * 2) - 2.0) { continue; }\n"
"            \n"
"            let dx = getValBilinear(scale, sample_x+1.0, sample_y) - getValBilinear(scale, sample_x-1.0, sample_y);\n"
"            let dy = getValBilinear(scale, sample_x, sample_y+1.0) - getValBilinear(scale, sample_x, sample_y-1.0);\n"
"            \n"
"            let mag = sqrt(dx*dx + dy*dy);\n"
"            let ori = atan2(dy, dx) - angle;\n"
"            \n"
"            var n_ori = ori;\n"
"            while (n_ori < 0.0) { n_ori += TWO_PI; }\n"
"            while (n_ori >= TWO_PI) { n_ori -= TWO_PI; }\n"
"            \n"
"            // Trilinear Interpolation\n"
"            // -0.5 to center the 4x4 bins (range 0-4 covers -8 to 8 pixels)\n"
"            let rbin = (f32(r) + 8.0) / f32(DESC_SUBGRID_SIZE) - 0.5;\n"
"            let cbin = (f32(c) + 8.0) / f32(DESC_SUBGRID_SIZE) - 0.5;\n"
"            let obin = n_ori * f32(DESC_BINS) / TWO_PI;\n"
"            \n"
"            let mag_w = mag * exp(-(f32(r*r + c*c)) / DESC_GAUSSIAN_WEIGHT_SIGMA_SQ);\n"
"            \n"
"            for (var dr = 0; dr < 2; dr++) {\n"
"                let ri = i32(floor(rbin)) + dr;\n"
"                if (ri >= 0 && ri < 4) {\n"
"                    let r_w = select(1.0 - fract(rbin), fract(rbin), dr == 1);\n"
"                    \n"
"                    for (var dc = 0; dc < 2; dc++) {\n"
"                        let ci = i32(floor(cbin)) + dc;\n"
"                        if (ci >= 0 && ci < 4) {\n"
"                            let c_w = select(1.0 - fract(cbin), fract(cbin), dc == 1);\n"
"                            \n"
"                            for (var do_idx = 0; do_idx < 2; do_idx++) {\n"
"                                let oi_raw = i32(floor(obin)) + do_idx;\n"
"                                let o_w = select(1.0 - fract(obin), fract(obin), do_idx == 1);\n"
"                                \n"
"                                let oi = (oi_raw + i32(DESC_BINS)) % i32(DESC_BINS);\n"
"                                let idx = (ri * i32(DESC_SUBGRID_SIZE) + ci) * i32(DESC_BINS) + oi;\n"
"                                desc[idx] += mag_w * r_w * c_w * o_w;\n"
"                            }\n"
"                        }\n"
"                    }\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"\n"
"    \n"
"    var norm = 0.0;\n"
"    for (var k=0u; k<DESC_DIM; k++) { norm += desc[k]*desc[k]; }\n"
"    norm = sqrt(norm) + 0.00001;\n"
"    \n"
"    for (var k=0u; k<DESC_DIM; k++) {\n"
"        desc[k] = min(0.2, desc[k] / norm);\n"
"    }\n"
"    \n"
"    norm = 0.0;\n"
"    for (var k=0u; k<DESC_DIM; k++) { norm += desc[k]*desc[k]; }\n"
"    norm = sqrt(norm) + 0.00001;\n"
"    \n"
"    for (var k=0u; k<DESC_DIM; k++) {\n"
"        descriptors.data[idx * DESC_DIM + k] = desc[k] / norm;\n"
"    }\n"
"}\n"
"" },
        { "packed/descriptor_quantized.wgsl", "struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"\n"
"struct DescriptorListQuantized {\n"
"    data: array<u32> // Packed 4x uint8 per u32\n"
"}\n"
"struct Params {\n"
"    width: i32, height: i32, octave: i32, pad: i32\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read_write> keypoints: KeypointList;\n"
"@group(0) @binding(2) var<storage, read_write> descriptors: DescriptorListQuantized;\n"
"@group(0) @binding(3) var tex1: texture_2d<f32>;\n"
"@group(0) @binding(4) var tex2: texture_2d<f32>;\n"
"@group(0) @binding(5) var tex3: texture_2d<f32>;\n"
"\n"
"fn getVal(s: i32, lx: i32, ly: i32) -> f32 {\n"
"    let px = lx / 2;\n"
"    let py = ly / 2;\n"
"    let mx = lx % 2;\n"
"    let my = ly % 2;\n"
"    \n"
"    var val: vec4f;\n"
"    if (s == 1) { val = textureLoad(tex1, vec2i(px, py), 0); }\n"
"    else if (s == 2) { val = textureLoad(tex2, vec2i(px, py), 0); }\n"
"    else { val = textureLoad(tex3, vec2i(px, py), 0); }\n"
"    \n"
"    if (my == 0) {\n"
"        return select(val.x, val.y, mx == 1);\n"
"    } else {\n"
"        return select(val.z, val.w, mx == 1);\n"
"    }\n"
"}\n"
"\n"
"fn getValBilinear(s: i32, x: f32, y: f32) -> f32 {\n"
"    let x0 = i32(floor(x));\n"
"    let y0 = i32(floor(y));\n"
"    let wx = x - f32(x0);\n"
"    let wy = y - f32(y0);\n"
"    \n"
"    let v00 = getVal(s, x0, y0);\n"
"    let v10 = getVal(s, x0 + 1, y0);\n"
"    let v01 = getVal(s, x0, y0 + 1);\n"
"    let v11 = getVal(s, x0 + 1, y0 + 1);\n"
"    \n"
"    return mix(mix(v00, v10, wx), mix(v01, v11, wx), wy);\n"
"}\n"
"\n"
"@compute @workgroup_size(64)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let idx = gid.x;\n"
"    if (idx >= atomicLoad(&keypoints.count)) { return; }\n"
"\n"
"    let kp = keypoints.points[idx];\n"
"    if (i32(kp.octave) != params.octave) { return; }\n"
"\n"
"    let x = kp.x / pow(2.0, kp.octave);\n"
"    let y = kp.y / pow(2.0, kp.octave);\n"
"    let scale = i32(kp.scale);\n"
"    let angle = kp.orientation;\n"
"    let cos_t = cos(angle);\n"
"    let sin_t = sin(angle);\n"
"    \n"
"    let sigma = SIGMA_BASE * pow(2.0, f32(scale)/SCALES_PER_OCTAVE);\n"
"    let step = DESC_STEP_FACTOR * sigma;\n"
"\n"
"    var desc = array<f32, DESC_DIM>();\n"
"    for (var k=0u; k<DESC_DIM; k++) { desc[k] = 0.0; }\n"
"\n"
"    for (var r = -8; r < 8; r++) {\n"
"        for (var c = -8; c < 8; c++) {\n"
"            let rot_x = step * (f32(c)*cos_t - f32(r)*sin_t);\n"
"            let rot_y = step * (f32(c)*sin_t + f32(r)*cos_t);\n"
"            let sample_x = x + rot_x;\n"
"            let sample_y = y + rot_y;\n"
"            \n"
"            if (sample_x < 2.0 || sample_x >= f32(params.width * 2) - 2.0 || \n"
"                sample_y < 2.0 || sample_y >= f32(params.height * 2) - 2.0) { continue; }\n"
"            \n"
"            let dx = getValBilinear(scale, sample_x+1.0, sample_y) - getValBilinear(scale, sample_x-1.0, sample_y);\n"
"            let dy = getValBilinear(scale, sample_x, sample_y+1.0) - getValBilinear(scale, sample_x, sample_y-1.0);\n"
"            \n"
"            let mag = sqrt(dx*dx + dy*dy);\n"
"            var ori = atan2(dy, dx) - angle;\n"
"            while (ori < 0.0) { ori += TWO_PI; }\n"
"            while (ori >= TWO_PI) { ori -= TWO_PI; }\n"
"            \n"
"            let rbin = (f32(r) + 8.0) / f32(DESC_SUBGRID_SIZE) - 0.5;\n"
"            let cbin = (f32(c) + 8.0) / f32(DESC_SUBGRID_SIZE) - 0.5;\n"
"            let obin = ori * f32(DESC_BINS) / TWO_PI;\n"
"            let mag_w = mag * exp(-(f32(r*r + c*c)) / DESC_GAUSSIAN_WEIGHT_SIGMA_SQ);\n"
"            \n"
"            for (var dr = 0; dr < 2; dr++) {\n"
"                let ri = i32(floor(rbin)) + dr;\n"
"                if (ri >= 0 && ri < 4) {\n"
"                    let r_w = select(1.0 - fract(rbin), fract(rbin), dr == 1);\n"
"                    for (var dc = 0; dc < 2; dc++) {\n"
"                        let ci = i32(floor(cbin)) + dc;\n"
"                        if (ci >= 0 && ci < 4) {\n"
"                            let c_w = select(1.0 - fract(cbin), fract(cbin), dc == 1);\n"
"                            for (var do_idx = 0; do_idx < 2; do_idx++) {\n"
"                                let oi_raw = i32(floor(obin)) + do_idx;\n"
"                                let o_w = select(1.0 - fract(obin), fract(obin), do_idx == 1);\n"
"                                let oi = (oi_raw + i32(DESC_BINS)) % i32(DESC_BINS);\n"
"                                let d_idx = (ri * i32(DESC_SUBGRID_SIZE) + ci) * i32(DESC_BINS) + oi;\n"
"                                desc[d_idx] += mag_w * r_w * c_w * o_w;\n"
"                            }\n"
"                        }\n"
"                    }\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"\n"
"    var norm = 0.0;\n"
"    for (var k=0u; k<DESC_DIM; k++) { norm += desc[k]*desc[k]; }\n"
"    norm = sqrt(norm) + 0.00001;\n"
"    for (var k=0u; k<DESC_DIM; k++) { desc[k] = min(0.2, desc[k] / norm); }\n"
"    norm = 0.0;\n"
"    for (var k=0u; k<DESC_DIM; k++) { norm += desc[k]*desc[k]; }\n"
"    norm = sqrt(norm) + 0.00001;\n"
"    \n"
"    for (var k=0u; k<32u; k++) {\n"
"        let v1 = u32(clamp(desc[k*4u+0u] / norm * 512.0, 0.0, 255.0));\n"
"        let v2 = u32(clamp(desc[k*4u+1u] / norm * 512.0, 0.0, 255.0));\n"
"        let v3 = u32(clamp(desc[k*4u+2u] / norm * 512.0, 0.0, 255.0));\n"
"        let v4 = u32(clamp(desc[k*4u+3u] / norm * 512.0, 0.0, 255.0));\n"
"        descriptors.data[idx * 32u + k] = v1 | (v2 << 8u) | (v3 << 16u) | (v4 << 24u);\n"
"    }\n"
"}\n"
"" },
        { "packed/dog.wgsl", "@group(0) @binding(0) var texA: texture_2d<f32>; // Packed RGBA\n"
"@group(0) @binding(1) var texB: texture_2d<f32>; // Packed RGBA\n"
"@group(0) @binding(2) var outputTex: texture_storage_2d<rgba32float, write>;\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(WG_SIZE_X, WG_SIZE_Y)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let dims = textureDimensions(texA);\n"
"    if (gid.x >= dims.x || gid.y >= dims.y) { return; }\n"
"    \n"
"    let a = textureLoad(texA, vec2i(gid.xy), 0);\n"
"    let b = textureLoad(texB, vec2i(gid.xy), 0);\n"
"    \n"
"    // Vectorized subtract\n"
"    textureStore(outputTex, vec2i(gid.xy), a - b);\n"
"}\n"
"" },
        { "packed/downsample.wgsl", "struct Params {\n"
"    srcWidth: u32,\n"
"    srcHeight: u32,\n"
"    dstWidth: u32,\n"
"    dstHeight: u32,\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var inputTex: texture_2d<f32>;\n"
"@group(0) @binding(2) var outputTex: texture_storage_2d<rgba32float, write>;\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(WG_SIZE_X, WG_SIZE_Y)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    if (gid.x >= params.dstWidth || gid.y >= params.dstHeight) { return; }\n"
"    \n"
"    // Destination is packed pixel (dx, dy) -> logical 2x2 block.\n"
"    // Logical coords: (2dx, 2dy), (2dx+1, 2dy), ...\n"
"    // Source coords should be 2x these logical coords?\n"
"    // Downsample is usually by picking every 2nd pixel (0, 2, 4...)\n"
"    // So Logical Src(sx, sy) = Logical Dst(dx, dy) * 2\n"
"    \n"
"    // Dst Pixel Components:\n"
"    // .x (TopLeft) -> Logical Dst(2dx, 2dy)     -> Src(4dx, 4dy)\n"
"    // .y (TopRight)-> Logical Dst(2dx+1, 2dy)   -> Src(4dx+2, 4dy)\n"
"    // .z (BotLeft) -> Logical Dst(2dx, 2dy+1)   -> Src(4dx, 4dy+2)\n"
"    // .w (BotRight)-> Logical Dst(2dx+1, 2dy+1) -> Src(4dx+2, 4dy+2)\n"
"    \n"
"    // Now map Logical Src to Packed Src:\n"
"    // P_Src(X, Y) contains Logical(2X..2X+1, 2Y..2Y+1)\n"
"    \n"
"    // 1. Src(4dx, 4dy):\n"
"    //    Packed X = 4dx / 2 = 2dx. Mod = 0.\n"
"    //    Packed Y = 4dy / 2 = 2dy. Mod = 0.\n"
"    //    Load Packed(2dx, 2dy). Component .x (TL)\n"
"    \n"
"    // 2. Src(4dx+2, 4dy):\n"
"    //    Packed X = (4dx+2)/2 = 2dx+1.\n"
"    //    Packed Y = 2dy.\n"
"    //    Load Packed(2dx+1, 2dy). Component .x (TL)\n"
"    \n"
"    // 3. Src(4dx, 4dy+2):\n"
"    //    Packed X = 2dx.\n"
"    //    Packed Y = 2dy+1.\n"
"    //    Load Packed(2dx, 2dy+1). Component .x (TL)\n"
"    \n"
"    // 4. Src(4dx+2, 4dy+2):\n"
"    //    Packed X = 2dx+1.\n"
"    //    Packed Y = 2dy+1.\n"
"    //    Load Packed(2dx+1, 2dy+1). Component .x (TL)\n"
"    \n"
"    // It seems we always sample the TL component (.x) of the packed source pixels!\n"
"    // Because we are downsampling by 2, and the packing is by 2.\n"
"    // So we skip every other PACKED pixel?\n"
"    // No, we skip every other LOGICAL pixel.\n"
"    // Logical indices: 0, 1, 2, 3, 4, 5...\n"
"    // Keep: 0, 2, 4...\n"
"    // Packed indices: \n"
"    //   P0=[0,1], P1=[2,3], P2=[4,5]\n"
"    //   We want 0 (from P0.x), 2 (from P1.x), 4 (from P2.x).\n"
"    //   We skip P0.y (1), P0.z (row 1), P0.w\n"
"    //   Wait, row indices also skip.\n"
"    //   So yes, we only read .x components from specific packed pixels.\n"
"    \n"
"    let sx = gid.x * 2u;\n"
"    let sy = gid.y * 2u;\n"
"    \n"
"    let v0 = textureLoad(inputTex, vec2i(i32(sx), i32(sy)), 0).x;     // TL\n"
"    let v1 = textureLoad(inputTex, vec2i(i32(sx+1), i32(sy)), 0).x;   // TR\n"
"    let v2 = textureLoad(inputTex, vec2i(i32(sx), i32(sy+1)), 0).x;   // BL\n"
"    let v3 = textureLoad(inputTex, vec2i(i32(sx+1), i32(sy+1)), 0).x; // BR\n"
"    \n"
"    textureStore(outputTex, vec2i(gid.xy), vec4f(v0, v1, v2, v3));\n"
"}\n"
"" },
        { "packed/extrema.wgsl", "struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"\n"
"struct Params { width: i32, height: i32, octave: i32, scale: i32, threshold: f32, edgeThreshold: f32 }\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var prevTex: texture_2d<f32>; // Packed\n"
"@group(0) @binding(2) var currTex: texture_2d<f32>; // Packed\n"
"@group(0) @binding(3) var nextTex: texture_2d<f32>; // Packed\n"
"@group(0) @binding(4) var<storage, read_write> keypoints: KeypointList;\n"
"\n"
"// Helper to Sample Logical Pixel (lx, ly)\n"
"fn getVal(tex: texture_2d<f32>, lx: i32, ly: i32) -> f32 {\n"
"    let px = lx / 2;\n"
"    let py = ly / 2;\n"
"    let mx = lx % 2;\n"
"    let my = ly % 2;\n"
"    let val = textureLoad(tex, vec2i(px, py), 0);\n"
"    \n"
"    // Select component\n"
"    if (my == 0) {\n"
"        return select(val.x, val.y, mx == 1);\n"
"    } else {\n"
"        return select(val.z, val.w, mx == 1);\n"
"    }\n"
"}\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(WG_SIZE_X, WG_SIZE_Y)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u, @builtin(local_invocation_index) lid: u32) {\n"
"    workgroupBarrier();\n"
"\n"
"    // gid is PACKED coordinates. \n"
"    // We process 4 logical pixels per thread? Or 1 logical pixel per thread?\n"
"    // If we map 1 thread -> 1 packed pixel:\n"
"    // It processes logical (2x, 2y), (2x+1, 2y)...\n"
"    // This is efficient.\n"
"    \n"
"    let px = i32(gid.x);\n"
"    let py = i32(gid.y);\n"
"    \n"
"    // Iterate 4 sub-pixels\n"
"    for (var sy = 0; sy < 2; sy++) {\n"
"        for (var sx = 0; sx < 2; sx++) {\n"
"            let lx = px * 2 + sx; // Logical X\n"
"            let ly = py * 2 + sy; // Logical Y\n"
"            \n"
"            // Check logical bounds\n"
"            if (lx < 1 || ly < 1 || lx >= params.width * 2 - 1 || ly >= params.height * 2 - 1) { continue; }\n"
"            \n"
"            checkKeypoint(lx, ly);\n"
"        }\n"
"    }\n"
"    \n"
"    // ... Aggregation logic (same as opt) ...\n"
"    // Note: wgCount is shared, so multiple `checkKeypoint` calls can increment it.\n"
"}\n"
"\n"
"fn checkKeypoint(x: i32, y: i32) {\n"
"    // Standard checks using getVal()\n"
"    let val = getVal(currTex, x, y);\n"
"    if (abs(val) < params.threshold) { return; }\n"
"    \n"
"    var isMax = true;\n"
"    var isMin = true;\n"
"    \n"
"    // 3x3x3 Neighbor check\n"
"    for (var vz = -1; vz <= 1; vz++) {\n"
"        for (var vy = -1; vy <= 1; vy++) {\n"
"            for (var vx = -1; vx <= 1; vx++) {\n"
"                if (vz == 0 && vy == 0 && vx == 0) { continue; }\n"
"                var nVal: f32;\n"
"                if (vz == -1) { nVal = getVal(prevTex, x+vx, y+vy); }\n"
"                else if (vz == 0) { nVal = getVal(currTex, x+vx, y+vy); }\n"
"                else { nVal = getVal(nextTex, x+vx, y+vy); }\n"
"                \n"
"                if (nVal >= val) { isMax = false; }\n"
"                if (nVal <= val) { isMin = false; }\n"
"            }\n"
"        }\n"
"    }\n"
"    \n"
"    if (!isMax && !isMin) { return; }\n"
"    \n"
"    // Edge Check\n"
"    let dxx = getVal(currTex, x+1, y) + getVal(currTex, x-1, y) - 2.0 * val;\n"
"    let dyy = getVal(currTex, x, y+1) + getVal(currTex, x, y-1) - 2.0 * val;\n"
"    let dxy = (getVal(currTex, x+1, y+1) - getVal(currTex, x+1, y-1) - getVal(currTex, x-1, y+1) + getVal(currTex, x-1, y-1)) * 0.25;\n"
"    // Edge check: Reject points that have a large principal curvature in one direction\n"
"    // but a small one in the other (edges).\n"
"    // Uses the ratio of eigenvalues of the 2x2 Hessian matrix.\n"
"    let tr = dxx + dyy;\n"
"    let det = dxx * dyy - dxy * dxy;\n"
"    let r = params.edgeThreshold;\n"
"    \n"
"    if (det > 0.0 && (tr * tr * r) < ((r + 1.0) * (r + 1.0) * det)) {\n"
"        // Add Keypoint\n"
"        let globalIdx = atomicAdd(&keypoints.count, 1u);\n"
"        let scaleMult = pow(2.0, f32(params.octave));\n"
"        \n"
"        keypoints.points[globalIdx].x = f32(x) * scaleMult;\n"
"        keypoints.points[globalIdx].y = f32(y) * scaleMult;\n"
"        keypoints.points[globalIdx].octave = f32(params.octave);\n"
"        keypoints.points[globalIdx].scale = f32(params.scale);\n"
"        keypoints.points[globalIdx].sigma = SIGMA_BASE * pow(2.0, (f32(params.scale) / SCALES_PER_OCTAVE)) * scaleMult;\n"
"        keypoints.points[globalIdx].orientation = 0.0;\n"
"    }\n"
"}\n"
"" },
        { "packed/grayscale.wgsl", "@group(0) @binding(0) var inputTex: texture_2d<f32>;\n"
"@group(0) @binding(1) var outputTex: texture_storage_2d<rgba32float, write>;\n"
"\n"
"// Weights for RGB -> Grayscale\n"
"const W = vec3f(0.299, 0.587, 0.114);\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE_X: u32 = 16u;\n"
"override WG_SIZE_Y: u32 = 16u;\n"
"\n"
"// 16x16 workgroup size (256 threads) is a balanced choice for 2D image processing\n"
"// ensuring high occupancy and efficient texture access across most GPU architectures.\n"
"@compute @workgroup_size(WG_SIZE_X, WG_SIZE_Y)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let dims = textureDimensions(inputTex);\n"
"    // gid describes the PACKED coordinates (w/2, h/2)\n"
"    let px = i32(gid.x);\n"
"    let py = i32(gid.y);\n"
"    \n"
"    // Check bounds of output texture\n"
"    let outDims = textureDimensions(outputTex);\n"
"    if (px >= i32(outDims.x) || py >= i32(outDims.y)) { return; }\n"
"\n"
"    // Src coords (2x2 block)\n"
"    let sx = px * 2;\n"
"    let sy = py * 2;\n"
"    \n"
"    // Load 4 pixels\n"
"    // Clamp to input dims if odd size - textureLoad handles out of bounds by return 0? No, usually clamped or needs check.\n"
"    // Safe check:\n"
"    let v00 = textureLoad(inputTex, vec2i(sx, sy), 0);\n"
"    let v10 = textureLoad(inputTex, vec2i(sx+1, sy), 0);\n"
"    let v01 = textureLoad(inputTex, vec2i(sx, sy+1), 0);\n"
"    let v11 = textureLoad(inputTex, vec2i(sx+1, sy+1), 0);\n"
"    \n"
"    // Convert to gray\n"
"    let g00 = dot(v00.rgb, W);\n"
"    let g10 = dot(v10.rgb, W);\n"
"    let g01 = dot(v01.rgb, W);\n"
"    let g11 = dot(v11.rgb, W);\n"
"    \n"
"    // Pack: x=(0,0), y=(1,0), z=(0,1), w=(1,1)\n"
"    // Corresponds to: TopLeft, TopRight, BotLeft, BotRight\n"
"    textureStore(outputTex, vec2i(px, py), vec4f(g00, g10, g01, g11));\n"
"}\n"
"" },
        { "packed/orientation.wgsl", "struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"\n"
"struct Params {\n"
"    width: i32, height: i32, octave: i32\n"
"}\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read_write> keypoints: KeypointList;\n"
"@group(0) @binding(2) var tex1: texture_2d<f32>;\n"
"@group(0) @binding(3) var tex2: texture_2d<f32>;\n"
"@group(0) @binding(4) var tex3: texture_2d<f32>;\n"
"\n"
"var<workgroup> wgHist: array<atomic<u32>, ORI_BINS>;\n"
"\n"
"fn getVal(s: i32, lx: i32, ly: i32) -> f32 {\n"
"    let px = lx / 2;\n"
"    let py = ly / 2;\n"
"    let mx = lx % 2;\n"
"    let my = ly % 2;\n"
"    \n"
"    var val: vec4f;\n"
"    if (s == 1) { val = textureLoad(tex1, vec2i(px, py), 0); }\n"
"    else if (s == 2) { val = textureLoad(tex2, vec2i(px, py), 0); }\n"
"    else { val = textureLoad(tex3, vec2i(px, py), 0); }\n"
"    \n"
"    if (my == 0) {\n"
"        return select(val.x, val.y, mx == 1);\n"
"    } else {\n"
"        return select(val.z, val.w, mx == 1);\n"
"    }\n"
"}\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE: u32 = 256u;\n"
"\n"
"// 256 threads (1D) provides high occupancy and matches the thread count of 2D kernels (16x16).\n"
"// This is suitable for processing lists of keypoints.\n"
"@compute @workgroup_size(WG_SIZE)\n"
"fn main(@builtin(workgroup_id) wid: vec3u, @builtin(local_invocation_index) lid: u32) {\n"
"    let idx = wid.x + wid.y * 65535u;\n"
"    \n"
"    if (lid < ORI_BINS) {\n"
"        atomicStore(&wgHist[lid], 0u);\n"
"    }\n"
"    workgroupBarrier();\n"
"\n"
"    let idxValid = idx < atomicLoad(&keypoints.count);\n"
"    var kp_octave = 0.0;\n"
"    var kp_x = 0.0;\n"
"    var kp_y = 0.0;\n"
"    var kp_scale = 0.0;\n"
"    \n"
"    if (idxValid) {\n"
"        let kp = keypoints.points[idx];\n"
"        kp_octave = kp.octave;\n"
"        kp_x = kp.x;\n"
"        kp_y = kp.y;\n"
"        kp_scale = kp.scale;\n"
"    }\n"
"    \n"
"    let isValid = idxValid && (i32(kp_octave) == params.octave);\n"
"\n"
"    var x = 0;\n"
"    var y = 0;\n"
"    var scale = 0;\n"
"    var sigma = 0.0;\n"
"    var radius = 0;\n"
"    var radiusSq = 0.0;\n"
"    var width = 0;\n"
"    var totalPixels = 0;\n"
"    \n"
"    if (isValid) {\n"
"        x = i32(round(kp_x / pow(2.0, kp_octave)));\n"
"        y = i32(round(kp_y / pow(2.0, kp_octave)));\n"
"        scale = i32(kp_scale);\n"
"        \n"
"        sigma = SIGMA_BASE * pow(2.0, f32(scale) / SCALES_PER_OCTAVE); \n"
"        radius = i32(round(sigma * 1.5 * 3.0));\n"
"        radiusSq = f32(radius * radius);\n"
"        width = 2 * radius + 1;\n"
"        totalPixels = width * width;\n"
"    }\n"
"    \n"
"    if (isValid && totalPixels > 0) {\n"
"        for (var i = i32(lid); i < totalPixels; i += 256) {\n"
"            let dy = (i / width) - radius;\n"
"            let dx = (i % width) - radius;\n"
"            \n"
"            let r2 = f32(dx*dx + dy*dy);\n"
"            if (r2 <= radiusSq) {\n"
"                let px = x + dx;\n"
"                let py = y + dy;\n"
"                // Check bounds (logical)\n"
"                if (px >= 1 && px < params.width * 2 - 1 && py >= 1 && py < params.height * 2 - 1) {\n"
"                    let rx = getVal(scale, px+1, py) - getVal(scale, px-1, py);\n"
"                    let ry = getVal(scale, px, py+1) - getVal(scale, px, py-1);\n"
"                    let mag = sqrt(rx*rx + ry*ry);\n"
"                    let sigma_w = 1.5 * sigma;\n"
"                    let weight = exp(-r2 / (2.0 * sigma_w * sigma_w));\n"
"                    \n"
"                    let ang_raw = atan2(ry, rx);\n"
"                    let ang = select(ang_raw, ang_raw + TWO_PI, ang_raw < 0.0);\n"
"                    let bin = i32(floor(ang * f32(ORI_BINS) / TWO_PI)) % i32(ORI_BINS);\n"
"                    \n"
"                    atomicAdd(&wgHist[bin], u32(mag * weight * HIST_SCALE));\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"    workgroupBarrier();\n"
"\n"
"    if (lid == 0u && isValid) {\n"
"        // Read histogram\n"
"        var rawHist = array<f32, ORI_BINS>();\n"
"        for (var i = 0; i < i32(ORI_BINS); i++) {\n"
"            rawHist[i] = f32(atomicLoad(&wgHist[i])) / HIST_SCALE;\n"
"        }\n"
"\n"
"        // Smooth Histogram [0.25, 0.5, 0.25]\n"
"        var histFloats = array<f32, ORI_BINS>();\n"
"        for (var i = 0; i < i32(ORI_BINS); i++) {\n"
"            let prev = rawHist[(i + i32(ORI_BINS) - 1) % i32(ORI_BINS)];\n"
"            let curr = rawHist[i];\n"
"            let next = rawHist[(i + 1) % i32(ORI_BINS)];\n"
"            histFloats[i] = 0.25 * prev + 0.5 * curr + 0.25 * next;\n"
"        }\n"
"\n"
"        var maxVal = -1.0;\n"
"        var maxBin = 0;\n"
"        \n"
"        for (var i = 0; i < i32(ORI_BINS); i++) {\n"
"            let val = histFloats[i];\n"
"            if (val > maxVal) {\n"
"                maxVal = val;\n"
"                maxBin = i;\n"
"            }\n"
"        }\n"
"        \n"
"        let left = histFloats[(maxBin + i32(ORI_BINS) - 1) % i32(ORI_BINS)];\n"
"        let right = histFloats[(maxBin + 1) % i32(ORI_BINS)];\n"
"        let peak = f32(maxBin) + 0.5 * (left - right) / (left - 2.0 * maxVal + right);\n"
"        let orientation = peak * TWO_PI / f32(ORI_BINS);\n"
"        \n"
"        keypoints.points[idx].orientation = orientation;\n"
"    }\n"
"}\n"
"" },
        { "matcher.wgsl", "struct Params {\n"
"    countA: u32,\n"
"    countB: u32,\n"
"    pad1: u32,\n"
"    pad2: u32\n"
"}\n"
"\n"
"struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read> descriptorsA: array<f32>;\n"
"@group(0) @binding(2) var<storage, read> descriptorsB: array<f32>;\n"
"@group(0) @binding(3) var<storage, read_write> results: array<MatchResult>;\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE: u32 = 64u;\n"
"\n"
"// 64 threads per workgroup balances parallelism and register pressure for descriptor matching.\n"
"@compute @workgroup_size(WG_SIZE)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let idxA = gid.x;\n"
"    if (idxA >= params.countA) { return; }\n"
"\n"
"    var bestDistSq = 1e30; // Infinity\n"
"    var secondDistSq = 1e30;\n"
"    var bestIdx = -1;\n"
"\n"
"    // Loop over all descriptors in set B\n"
"    for (var i = 0u; i < params.countB; i++) {\n"
"        var distSq = 0.0;\n"
"        \n"
"        // Compute Euclidean distance squared (128 dimensions)\n"
"        // Loop unrolling might help, but let's keep it simple\n"
"        for (var k = 0u; k < DESC_DIM; k++) {\n"
"            let valA = descriptorsA[idxA * DESC_DIM + k];\n"
"            let valB = descriptorsB[i * DESC_DIM + k];\n"
"            let diff = valA - valB;\n"
"            distSq += diff * diff;\n"
"        }\n"
"\n"
"        if (distSq < bestDistSq) {\n"
"            secondDistSq = bestDistSq;\n"
"            bestDistSq = distSq;\n"
"            bestIdx = i32(i);\n"
"        } else if (distSq < secondDistSq) {\n"
"            secondDistSq = distSq;\n"
"        }\n"
"    }\n"
"\n"
"    results[idxA].bestIdx = bestIdx;\n"
"    results[idxA].bestDistSq = bestDistSq;\n"
"    results[idxA].secondDistSq = secondDistSq;\n"
"}\n"
"" },
        { "matcher_guided.wgsl", "struct Params {\n"
"    countA: u32,\n"
"    countB: u32,\n"
"    threshold: f32,\n"
"    pad: u32,\n"
"    col0: vec4f,\n"
"    col1: vec4f,\n"
"    col2: vec4f,\n"
"}\n"
"\n"
"struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read> descriptorsA: array<f32>;\n"
"@group(0) @binding(2) var<storage, read> descriptorsB: array<f32>;\n"
"@group(0) @binding(3) var<storage, read_write> results: array<MatchResult>;\n"
"@group(0) @binding(4) var<storage, read> keypointsA: array<vec2f>;\n"
"@group(0) @binding(5) var<storage, read> keypointsB: array<vec2f>;\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE: u32 = 64u;\n"
"\n"
"// 64 threads per workgroup balances parallelism and register pressure for descriptor matching.\n"
"@compute @workgroup_size(WG_SIZE)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let idxA = gid.x;\n"
"    if (idxA >= params.countA) { return; }\n"
"\n"
"    let pA = vec3f(keypointsA[idxA], 1.0);\n"
"    // Epipolar line in image B: l = F * pA\n"
"    // Since we pass F columns: l = col0*pA.x + col1*pA.y + col2*pA.z\n"
"    let lineB = params.col0.xyz * pA.x + params.col1.xyz * pA.y + params.col2.xyz * pA.z;\n"
"    \n"
"    // Pre-calculate line norm for distance\n"
"    let lineNorm = sqrt(lineB.x * lineB.x + lineB.y * lineB.y);\n"
"\n"
"    var bestDistSq = 1e30; \n"
"    var secondDistSq = 1e30;\n"
"    var bestIdx = -1;\n"
"\n"
"    for (var i = 0u; i < params.countB; i++) {\n"
"        let pB = keypointsB[i];\n"
"        \n"
"        // Epipolar distance: |lineB * pB| / lineNorm\n"
"        let distToLine = abs(lineB.x * pB.x + lineB.y * pB.y + lineB.z) / (lineNorm + 1e-6);\n"
"        \n"
"        if (distToLine > params.threshold) { continue; }\n"
"\n"
"        var distSq = 0.0;\n"
"        for (var k = 0u; k < DESC_DIM; k++) {\n"
"            let diff = descriptorsA[idxA * DESC_DIM + k] - descriptorsB[i * DESC_DIM + k];\n"
"            distSq += diff * diff;\n"
"        }\n"
"\n"
"        if (distSq < bestDistSq) {\n"
"            secondDistSq = bestDistSq;\n"
"            bestDistSq = distSq;\n"
"            bestIdx = i32(i);\n"
"        } else if (distSq < secondDistSq) {\n"
"            secondDistSq = distSq;\n"
"        }\n"
"    }\n"
"\n"
"    results[idxA].bestIdx = bestIdx;\n"
"    results[idxA].bestDistSq = bestDistSq;\n"
"    results[idxA].secondDistSq = secondDistSq;\n"
"}\n"
"" },
        { "matcher_quantized.wgsl", "struct Params {\n"
"    countA: u32,\n"
"    countB: u32,\n"
"    pad1: u32,\n"
"    pad2: u32\n"
"}\n"
"\n"
"struct Keypoint {\n"
"    x: f32, y: f32, octave: f32, scale: f32, sigma: f32, orientation: f32, p1: f32, p2: f32\n"
"}\n"
"struct KeypointList {\n"
"    count: atomic<u32>, pad1: u32, pad2: u32, pad3: u32, points: array<Keypoint>\n"
"}\n"
"struct MatchResult {\n"
"    bestIdx: i32,\n"
"    bestDistSq: f32,\n"
"    secondDistSq: f32,\n"
"    pad: f32\n"
"}\n"
"const PI: f32 = 3.141592653589793;\n"
"const TWO_PI: f32 = 6.283185307179586;\n"
"\n"
"const SIGMA_BASE: f32 = 1.6;\n"
"const SCALES_PER_OCTAVE: f32 = 3.0;\n"
"\n"
"const ORI_BINS: u32 = 36u;\n"
"const ORI_PEAK_RATIO: f32 = 0.8;\n"
"\n"
"const DESC_DIM: u32 = 128u;\n"
"const DESC_SUBGRID_SIZE: u32 = 4u;\n"
"const DESC_BINS: u32 = 8u;\n"
"const DESC_STEP_FACTOR: f32 = 0.75;\n"
"const DESC_GAUSSIAN_WEIGHT_SIGMA_SQ: f32 = 32.0;\n"
"\n"
"const HIST_SCALE: f32 = 1000.0;\n"
"\n"
"@group(0) @binding(0) var<uniform> params: Params;\n"
"@group(0) @binding(1) var<storage, read> descriptorsA: array<u32>;\n"
"@group(0) @binding(2) var<storage, read> descriptorsB: array<u32>;\n"
"@group(0) @binding(3) var<storage, read_write> results: array<MatchResult>;\n"
"\n"
"// Workgroup sizes can be specialized if needed for different GPU architectures.\n"
"override WG_SIZE: u32 = 64u;\n"
"\n"
"// 64 threads per workgroup balances parallelism and register pressure for descriptor matching.\n"
"@compute @workgroup_size(WG_SIZE)\n"
"fn main(@builtin(global_invocation_id) gid: vec3u) {\n"
"    let idxA = gid.x;\n"
"    if (idxA >= params.countA) { return; }\n"
"\n"
"    var bestDistSq = 1e30; \n"
"    var secondDistSq = 1e30;\n"
"    var bestIdx = -1;\n"
"\n"
"    for (var i = 0u; i < params.countB; i++) {\n"
"        var distSq = 0.0;\n"
"        \n"
"        for (var k = 0u; k < DESC_DIM / 4u; k++) {\n"
"            let valA = descriptorsA[idxA * (DESC_DIM / 4u) + k];\n"
"            let valB = descriptorsB[i * (DESC_DIM / 4u) + k];\n"
"            \n"
"            // Unpack 4 bytes manually\n"
"            let a1 = f32(valA & 0xFFu);\n"
"            let a2 = f32((valA >> 8u) & 0xFFu);\n"
"            let a3 = f32((valA >> 16u) & 0xFFu);\n"
"            let a4 = f32((valA >> 24u) & 0xFFu);\n"
"            \n"
"            let b1 = f32(valB & 0xFFu);\n"
"            let b2 = f32((valB >> 8u) & 0xFFu);\n"
"            let b3 = f32((valB >> 16u) & 0xFFu);\n"
"            let b4 = f32((valB >> 24u) & 0xFFu);\n"
"            \n"
"            let d1 = a1 - b1;\n"
"            let d2 = a2 - b2;\n"
"            let d3 = a3 - b3;\n"
"            let d4 = a4 - b4;\n"
"            \n"
"            distSq += d1*d1 + d2*d2 + d3*d3 + d4*d4;\n"
"        }\n"
"\n"
"        if (distSq < bestDistSq) {\n"
"            secondDistSq = bestDistSq;\n"
"            bestDistSq = distSq;\n"
"            bestIdx = i32(i);\n"
"        } else if (distSq < secondDistSq) {\n"
"            secondDistSq = distSq;\n"
"        }\n"
"    }\n"
"\n"
"    results[idxA].bestIdx = bestIdx;\n"
"    results[idxA].bestDistSq = bestDistSq;\n"
"    results[idxA].secondDistSq = secondDistSq;\n"
"}\n"
"" },
    };
    
    inline std::string GetShader(const std::string& path) {
        auto it = shaders.find(path);
        if (it != shaders.end()) return it->second;
        return "";
    }
}
